{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Phoneme Sequences\n",
    "Here I have to process the phoneme sequences so that they can be fed into a Keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7 20  6  0]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing for using whole-sequence embedding approach\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import csv\n",
    "import functools\n",
    "import pandas as pd\n",
    "import random\n",
    "import blist\n",
    "\n",
    "random.seed(285)\n",
    "\n",
    "# need to read in csv file with pairs and labels\n",
    "train = pd.read_csv(\"capstone_train_and_test/new_train.csv\")\n",
    "vocabulary_size = 39 #aka number of different phonemes\n",
    "max_len = 6 #maximum size of a phoneme sequence. Average sequence is 6.34 phonemes\n",
    "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(train[\"phonemic_transcriptions_1\"]) #finds number of tokens (phonemes in this case)\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train[\"phonemic_transcriptions_1\"]) #translates all words to lists of integers\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train[\"phonemic_transcriptions_2\"])\n",
    "train_data_1_concrete = pad_sequences(train_sequences_1, maxlen = max_len, padding = \"post\")\n",
    "train_data_2_concrete = pad_sequences(train_sequences_2, maxlen = max_len, padding = \"post\")\n",
    "print(train_data_1_concrete[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = train[\"rhyme_percentile\"]*10\n",
    "categorized_labels_concrete = to_categorical(raw_labels) # the `y` label we're trying to fit to\n",
    "train_data_1_concrete = list(train_data_1_concrete)\n",
    "train_data_2_concrete = list(train_data_2_concrete)\n",
    "categorized_labels_concrete = list(categorized_labels_concrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find info about lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Data process starting\n",
      "192127\n",
      "1117331\n",
      "2289063\n",
      "2279152\n",
      "1159709\n",
      "356762\n",
      "84146\n",
      "16621\n",
      "3401\n",
      "539\n",
      "2148\n",
      "Marking for deletion starting\n",
      "Marking for deletion finished\n",
      "Making filtered list starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:32: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:33: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making filtered list finished\n",
      "Reduce Data process finished\n",
      "50182\n",
      "50368\n",
      "49506\n",
      "49679\n",
      "50030\n",
      "50081\n",
      "49962\n",
      "16621\n",
      "3401\n",
      "539\n",
      "2148\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = train_data_1_concrete\n",
    "train_data_2 = train_data_2_concrete\n",
    "categorized_labels = categorized_labels_concrete\n",
    "\n",
    "def reduce_data():\n",
    "    global train_data_1\n",
    "    global train_data_2\n",
    "    global raw_labels\n",
    "    global categorized_labels\n",
    "    \n",
    "    #get number of words in each category\n",
    "    print(\"Reduce Data process starting\")\n",
    "    category_amounts = [0 for i in range(11)]\n",
    "    for i in range(len(raw_labels)):\n",
    "        category_amounts[int(raw_labels[i])] += 1\n",
    "    for i in range(len(category_amounts)):\n",
    "        print(category_amounts[i])\n",
    "    \n",
    "    #go through each sample and remove based on fraction\n",
    "    ceiling = 50000\n",
    "    print(\"Marking for deletion starting\")\n",
    "    for i in range(len(raw_labels)):\n",
    "        random_num = random.random()\n",
    "        fraction = 1/(category_amounts[int(raw_labels[i])] / ceiling)\n",
    "        if random_num >= fraction:\n",
    "            # delete it\n",
    "            train_data_1[i] = \"\"\n",
    "            train_data_2[i] = \"\"\n",
    "            categorized_labels[i] = [0.]\n",
    "    print(\"Marking for deletion finished\")\n",
    "    print(\"Making filtered list starting\")\n",
    "    train_data_1 = [t for t in train_data_1 if t != \"\"]\n",
    "    train_data_2 = [t for t in train_data_2 if t != \"\"]\n",
    "    categorized_labels = [t for t in categorized_labels if sum(t) != 0.]\n",
    "    print(\"Making filtered list finished\")\n",
    "    print(\"Reduce Data process finished\")\n",
    "\n",
    "reduce_data()\n",
    "category_amounts = [0 for i in range(11)]\n",
    "for i in range(len(categorized_labels)):\n",
    "    current_label = categorized_labels[i]\n",
    "    index = 0\n",
    "    for u in range(len(current_label)):\n",
    "        if current_label[u] == 1:\n",
    "            index = u\n",
    "    category_amounts[index] += 1\n",
    "for i in range(len(category_amounts)):\n",
    "    print(category_amounts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (100, 17)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (100, 17)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (100, 17, 20)        780         input_29[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (100, 8, 100)        6100        embedding_7[0][0]                \n",
      "                                                                 embedding_7[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (100, 3, 50)         15050       conv1d_31[0][0]                  \n",
      "                                                                 conv1d_31[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (100, 1, 25)         3775        conv1d_32[0][0]                  \n",
      "                                                                 conv1d_32[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (100, 1, 25)         0           conv1d_33[0][0]                  \n",
      "                                                                 conv1d_33[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (100, 25)            0           subtract_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (100, 11)            286         flatten_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,991\n",
      "Trainable params: 25,991\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build whole-sequence model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Subtract, Conv1D\n",
    "\n",
    "batch_size = 100\n",
    "output_dim_size = 20\n",
    "\n",
    "input_1 = Input(batch_shape=(batch_size, 17))\n",
    "input_2 = Input(batch_shape=(batch_size, 17))\n",
    "embedding = Embedding(vocabulary_size, output_dim_size, input_length=max_len)\n",
    "conv1d_1 = Conv1D(100, 3, activation = \"relu\", strides=2)\n",
    "conv1d_2 = Conv1D(50, 3, activation = \"relu\", strides=2)\n",
    "conv1d_3 = Conv1D(25, 3, activation = \"relu\", strides=2)\n",
    "\n",
    "embedding_1 = embedding(input_1)\n",
    "embedding_2 = embedding(input_2)\n",
    "conv1d_1_1 = conv1d_1(embedding_1)\n",
    "conv1d_1_2 = conv1d_1(embedding_2)\n",
    "conv1d_2_1 = conv1d_2(conv1d_1_1)\n",
    "conv1d_2_2 = conv1d_2(conv1d_1_2)\n",
    "conv1d_3_1 = conv1d_3(conv1d_2_1)\n",
    "conv1d_3_2 = conv1d_3(conv1d_2_2)\n",
    "merge_layer = Subtract()([conv1d_3_1, conv1d_3_2])\n",
    "flatten = Flatten()(merge_layer)\n",
    "dense_output_layer = Dense(11, activation=\"softmax\", input_shape=(max_len*output_dim_size,))(flatten)\n",
    "\n",
    "whole_sequence_model = Model([input_1, input_2], dense_output_layer)\n",
    "whole_sequence_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372517\n",
      "Train on 298000 samples, validate on 74500 samples\n",
      "Epoch 1/10\n",
      "298000/298000 [==============================] - 32s 109us/step - loss: 2.1245 - acc: 0.1829 - val_loss: 2.0456 - val_acc: 0.2133\n",
      "Epoch 2/10\n",
      "298000/298000 [==============================] - 29s 98us/step - loss: 2.0253 - acc: 0.2222 - val_loss: 2.0155 - val_acc: 0.2303\n",
      "Epoch 3/10\n",
      "298000/298000 [==============================] - 28s 94us/step - loss: 2.0059 - acc: 0.2317 - val_loss: 2.0051 - val_acc: 0.2311\n",
      "Epoch 4/10\n",
      "298000/298000 [==============================] - 30s 101us/step - loss: 1.9940 - acc: 0.2374 - val_loss: 1.9995 - val_acc: 0.2415\n",
      "Epoch 5/10\n",
      "298000/298000 [==============================] - 30s 100us/step - loss: 1.9864 - acc: 0.2400 - val_loss: 1.9904 - val_acc: 0.2403\n",
      "Epoch 6/10\n",
      "298000/298000 [==============================] - 33s 111us/step - loss: 1.9822 - acc: 0.2413 - val_loss: 1.9895 - val_acc: 0.2427\n",
      "Epoch 7/10\n",
      "298000/298000 [==============================] - 33s 112us/step - loss: 1.9788 - acc: 0.2439 - val_loss: 1.9897 - val_acc: 0.2385\n",
      "Epoch 8/10\n",
      "298000/298000 [==============================] - 35s 119us/step - loss: 1.9765 - acc: 0.2427 - val_loss: 1.9896 - val_acc: 0.2284\n",
      "Epoch 9/10\n",
      "167500/298000 [===============>..............] - ETA: 14s - loss: 1.9726 - acc: 0.2448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2324e8f6ac31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m whole_sequence_model.fit([np.array(train_data_1[:372500]), np.array(train_data_2[:372500])], \n\u001b[1;32m      5\u001b[0m                          \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorized_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m372500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                          epochs=10, batch_size=100, validation_split = 0.2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile and fit model\n",
    "print(len(train_data_1))\n",
    "whole_sequence_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "whole_sequence_model.fit([np.array(train_data_1[:372500]), np.array(train_data_2[:372500])], \n",
    "                         np.array(categorized_labels[:372500]), \n",
    "                         epochs=10, batch_size=100, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500999\n",
      "[24, 14, 2, 10, 19, 11]\n",
      "[24 14  2 10 19 11  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing if using phoneme embedding approach\n",
    "# preprocessing to make the actual phoneme embedding\n",
    "\n",
    "#tokenization\n",
    "words = []\n",
    "with open(\"transcriptions_data.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        words.append(row)\n",
    "words = np.array(words).flatten()\n",
    "\n",
    "vocabulary_size = 39 #aka number of different phonemes\n",
    "max_len = 17 #maximum size of a phoneme sequence\n",
    "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(words) #finds number of tokens (phonemes in this case)\n",
    "sequences = tokenizer.texts_to_sequences(words) #translates all words to lists of integers\n",
    "print(sequences[10])\n",
    "data = pad_sequences(sequences, maxlen = max_len, padding = \"post\")\n",
    "print(data[10])\n",
    "\n",
    "#okay, so I have my sequences preprocessed\n",
    "# wait, actually I need to read in each column of all the phoneme sequences matched up together and their categorizations and make the categorizations Keras-compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build phoneme embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use phoneme embedding in new model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
