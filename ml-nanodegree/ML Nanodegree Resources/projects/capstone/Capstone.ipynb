{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Phoneme Sequences\n",
    "Here I have to process the phoneme sequences so that they can be fed into a Keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7 20  6  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing for using whole-sequence embedding approach\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import csv\n",
    "import functools\n",
    "import pandas as pd\n",
    "\n",
    "# need to read in csv file with pairs and labels\n",
    "train = pd.read_csv(\"capstone_train_and_test/new_train.csv\")\n",
    "vocabulary_size = 39 #aka number of different phonemes\n",
    "max_len = 17 #maximum size of a phoneme sequence\n",
    "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(words) #finds number of tokens (phonemes in this case)\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train[\"phonemic_transcriptions_1\"]) #translates all words to lists of integers\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train[\"phonemic_transcriptions_2\"])\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen = max_len, padding = \"post\")\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen = max_len, padding = \"post\")\n",
    "print(train_data_1[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          4.0\n",
      "1          4.0\n",
      "2          1.0\n",
      "3          3.0\n",
      "4          3.0\n",
      "5          0.0\n",
      "6          4.0\n",
      "7          3.0\n",
      "8          2.0\n",
      "9          2.0\n",
      "10         1.0\n",
      "11         3.0\n",
      "12         2.0\n",
      "13         3.0\n",
      "14         2.0\n",
      "15         3.0\n",
      "16         0.0\n",
      "17         2.0\n",
      "18         8.0\n",
      "19         2.0\n",
      "20         3.0\n",
      "21         4.0\n",
      "22         1.0\n",
      "23         4.0\n",
      "24         2.0\n",
      "25         2.0\n",
      "26         4.0\n",
      "27         0.0\n",
      "28         4.0\n",
      "29         1.0\n",
      "          ... \n",
      "7500969    2.0\n",
      "7500970    3.0\n",
      "7500971    1.0\n",
      "7500972    4.0\n",
      "7500973    2.0\n",
      "7500974    1.0\n",
      "7500975    4.0\n",
      "7500976    2.0\n",
      "7500977    2.0\n",
      "7500978    3.0\n",
      "7500979    4.0\n",
      "7500980    1.0\n",
      "7500981    2.0\n",
      "7500982    2.0\n",
      "7500983    1.0\n",
      "7500984    3.0\n",
      "7500985    2.0\n",
      "7500986    4.0\n",
      "7500987    4.0\n",
      "7500988    3.0\n",
      "7500989    2.0\n",
      "7500990    3.0\n",
      "7500991    2.0\n",
      "7500992    1.0\n",
      "7500993    2.0\n",
      "7500994    2.0\n",
      "7500995    2.0\n",
      "7500996    2.0\n",
      "7500997    2.0\n",
      "7500998    5.0\n",
      "Name: rhyme_percentile, Length: 7500999, dtype: float64\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "raw_labels = train[\"rhyme_percentile\"]*10\n",
    "print(raw_labels)\n",
    "categorized_labels = to_categorical(raw_labels) # the `y` label we're trying to fit to\n",
    "print(categorized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build whole-sequence model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding\n",
    "#decide on proper merge layer to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500999\n",
      "[24, 14, 2, 10, 19, 11]\n",
      "[24 14  2 10 19 11  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing if using phoneme embedding approach\n",
    "# preprocessing to make the actual phoneme embedding\n",
    "\n",
    "#tokenization\n",
    "words = []\n",
    "with open(\"transcriptions_data.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        words.append(row)\n",
    "words = np.array(words).flatten()\n",
    "\n",
    "vocabulary_size = 39 #aka number of different phonemes\n",
    "max_len = 17 #maximum size of a phoneme sequence\n",
    "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(words) #finds number of tokens (phonemes in this case)\n",
    "sequences = tokenizer.texts_to_sequences(words) #translates all words to lists of integers\n",
    "print(sequences[10])\n",
    "data = pad_sequences(sequences, maxlen = max_len, padding = \"post\")\n",
    "print(data[10])\n",
    "\n",
    "#okay, so I have my sequences preprocessed\n",
    "# wait, actually I need to read in each column of all the phoneme sequences matched up together and their categorizations and make the categorizations Keras-compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build phoneme embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use phoneme embedding in new model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
