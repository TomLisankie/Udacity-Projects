{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Phoneme Sequences\n",
    "Here I have to process the phoneme sequences so that they can be fed into a Keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7 20  6  0]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing for using whole-sequence embedding approach\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import csv\n",
    "import functools\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(285)\n",
    "\n",
    "# need to read in csv file with pairs and labels\n",
    "train = pd.read_csv(\"capstone_train_and_test/new_train.csv\")\n",
    "vocabulary_size = 39 #aka number of different phonemes\n",
    "max_len = 6 #maximum size of a phoneme sequence. Average sequence is 6.34 phonemes\n",
    "tokenizer = Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(train[\"phonemic_transcriptions_1\"]) #finds number of tokens (phonemes in this case)\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train[\"phonemic_transcriptions_1\"]) #translates all words to lists of integers\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train[\"phonemic_transcriptions_2\"])\n",
    "train_data_1_concrete = pad_sequences(train_sequences_1, maxlen = max_len, padding = \"post\")\n",
    "train_data_2_concrete = pad_sequences(train_sequences_2, maxlen = max_len, padding = \"post\")\n",
    "print(train_data_1_concrete[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = train[\"rhyme_percentile\"]*10\n",
    "categorized_labels_concrete = to_categorical(raw_labels) # the `y` label we're trying to fit to\n",
    "train_data_1_concrete = list(train_data_1_concrete)\n",
    "train_data_2_concrete = list(train_data_2_concrete)\n",
    "categorized_labels_concrete = list(categorized_labels_concrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find info about lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Data process starting\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = train_data_1_concrete\n",
    "train_data_2 = train_data_2_concrete\n",
    "categorized_labels = categorized_labels_concrete\n",
    "\n",
    "def reduce_data():\n",
    "    global train_data_1\n",
    "    global train_data_2\n",
    "    global raw_labels\n",
    "    global categorized_labels\n",
    "    \n",
    "    #get number of words in each category\n",
    "    print(\"Reduce Data process starting\")\n",
    "    category_amounts = [0 for i in range(11)]\n",
    "    for i in range(len(raw_labels)):\n",
    "        category_amounts[int(raw_labels[i])] += 1\n",
    "    for i in range(len(category_amounts)):\n",
    "        print(category_amounts[i])\n",
    "    \n",
    "    #go through each sample and remove based on fraction\n",
    "    ceiling = 50000\n",
    "    print(\"Marking for deletion starting\")\n",
    "    for i in range(len(raw_labels)):\n",
    "        random_num = random.random()\n",
    "        fraction = 1/(category_amounts[int(raw_labels[i])] / ceiling)\n",
    "        if random_num >= fraction:\n",
    "            # delete it\n",
    "            train_data_1[i] = \"\"\n",
    "            train_data_2[i] = \"\"\n",
    "            categorized_labels[i] = [0.]\n",
    "    print(\"Marking for deletion finished\")\n",
    "    print(\"Making filtered list starting\")\n",
    "    train_data_1 = [t for t in train_data_1 if t != \"\"]\n",
    "    train_data_2 = [t for t in train_data_2 if t != \"\"]\n",
    "    categorized_labels = [t for t in categorized_labels if sum(t) != 0.]\n",
    "    print(\"Making filtered list finished\")\n",
    "    print(\"Reduce Data process finished\")\n",
    "\n",
    "reduce_data()\n",
    "category_amounts = [0 for i in range(11)]\n",
    "for i in range(len(categorized_labels)):\n",
    "    current_label = categorized_labels[i]\n",
    "    index = 0\n",
    "    for u in range(len(current_label)):\n",
    "        if current_label[u] == 1:\n",
    "            index = u\n",
    "    category_amounts[index] += 1\n",
    "for i in range(len(category_amounts)):\n",
    "    print(category_amounts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build whole-sequence model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Subtract, Conv1D\n",
    "\n",
    "batch_size = 100\n",
    "output_dim_size = 20\n",
    "\n",
    "input_1 = Input(batch_shape=(batch_size, 6))\n",
    "input_2 = Input(batch_shape=(batch_size, 6))\n",
    "embedding = Embedding(vocabulary_size, output_dim_size, input_length=max_len)\n",
    "conv1d_1 = Conv1D(100, 6, activation = \"relu\", strides=1, padding=\"valid\")\n",
    "conv1d_2 = Conv1D(50, 6, activation = \"relu\", strides=1, padding=\"valid\")\n",
    "conv1d_3 = Conv1D(25, 6, activation = \"relu\", strides=1, padding=\"valid\")\n",
    "\n",
    "embedding_1 = embedding(input_1)\n",
    "embedding_2 = embedding(input_2)\n",
    "conv1d_1_1 = conv1d_1(embedding_1)\n",
    "conv1d_1_2 = conv1d_1(embedding_2)\n",
    "conv1d_2_1 = conv1d_2(conv1d_1_1)\n",
    "conv1d_2_2 = conv1d_2(conv1d_1_2)\n",
    "conv1d_3_1 = conv1d_3(conv1d_2_1)\n",
    "conv1d_3_2 = conv1d_3(conv1d_2_2)\n",
    "merge_layer = Subtract()([conv1d_3_1, conv1d_3_2])\n",
    "flatten = Flatten()(merge_layer)\n",
    "dense_output_layer = Dense(11, activation=\"softmax\", input_shape=(max_len*output_dim_size,))(flatten)\n",
    "\n",
    "whole_sequence_model = Model([input_1, input_2], dense_output_layer)\n",
    "whole_sequence_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372517\n",
      "Train on 298000 samples, validate on 74500 samples\n",
      "Epoch 1/10\n",
      "298000/298000 [==============================] - 21s 72us/step - loss: 2.1387 - acc: 0.1746 - val_loss: 2.0624 - val_acc: 0.1997\n",
      "Epoch 2/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 2.0471 - acc: 0.2061 - val_loss: 2.0401 - val_acc: 0.2179\n",
      "Epoch 3/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 2.0284 - acc: 0.2198 - val_loss: 2.0275 - val_acc: 0.2185\n",
      "Epoch 4/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 2.0172 - acc: 0.2261 - val_loss: 2.0164 - val_acc: 0.2234\n",
      "Epoch 5/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 2.0089 - acc: 0.2305 - val_loss: 2.0126 - val_acc: 0.2266\n",
      "Epoch 6/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 2.0026 - acc: 0.2324 - val_loss: 2.0083 - val_acc: 0.2303\n",
      "Epoch 7/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 1.9977 - acc: 0.2353 - val_loss: 2.0030 - val_acc: 0.2317\n",
      "Epoch 8/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 1.9944 - acc: 0.2373 - val_loss: 2.0018 - val_acc: 0.2332\n",
      "Epoch 9/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 1.9910 - acc: 0.2380 - val_loss: 2.0004 - val_acc: 0.2296\n",
      "Epoch 10/10\n",
      "298000/298000 [==============================] - 21s 69us/step - loss: 1.9887 - acc: 0.2394 - val_loss: 1.9968 - val_acc: 0.2376\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit model\n",
    "print(len(train_data_1))\n",
    "whole_sequence_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "the_model = whole_sequence_model.fit([np.array(train_data_1[:372500]), np.array(train_data_2[:372500])], \n",
    "                         np.array(categorized_labels[:372500]), \n",
    "                         epochs=10, batch_size=100, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
